# -*- coding: utf-8 -*-
"""inceptionv3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v_2P37TixFGnmty-vfhfFgdxgqIG4dmq

# FGSM adversarial attack on TensorFlow InceptionV3

This Colab notebook demonstrates the Fast Gradient Sign Method (FGSM) attack against TensorFlow's pre-trained **InceptionV3** model (ImageNet weights). It:
- Loads a sample image from the web (ImageNet-like photo),
- Preprocesses for InceptionV3,
- Computes FGSM adversarial example for a chosen epsilon,
- Displays original and adversarial predictions and images.

Run this in Google Colab (enable a GPU runtime for speed).
"""

# Install / check TensorFlow version (Colab usually has TF installed; this cell is safe to run)
!pip install -q tensorflow
import tensorflow as tf
print('TensorFlow version:', tf.__version__)

from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions
from tensorflow.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from PIL import Image
import os

# Load model with ImageNet weights
model = InceptionV3(weights='imagenet')
model.trainable = False
model.summary()

import numpy as np
import matplotlib.pyplot as plt
from google.colab import files
from tensorflow.keras.preprocessing import image

# Helper: upload an image, resize to 299x299 (InceptionV3 input), and return tensor
def load_uploaded_image(target_size=(299,299)):
    uploaded = files.upload()  # prompts you to choose a file
    for fname in uploaded.keys():
        img_path = fname
    img = image.load_img(img_path, target_size=target_size)
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    return x, img

# Example usage
x, pil_img = load_uploaded_image()
plt.imshow(pil_img)
plt.axis('off')
plt.title('Uploaded image')
plt.show()

# Preprocess and predict original
x_proc = preprocess_input(x.copy())  # preprocess for InceptionV3
preds = model.predict(x_proc)
print('Top predictions for original image:')
for p in decode_predictions(preds, top=3)[0]:
    print(p)

"""## FGSM Attack implementation

We compute the gradient of the loss w.r.t. the input image and perturb the image in the direction of the sign of the gradient:
\[x_{adv} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))\]
We'll implement a small function to craft the adversarial example and show its prediction.
"""

loss_object = tf.keras.losses.CategoricalCrossentropy()

@tf.function
def create_adversarial_pattern(model, input_image, input_label):
    with tf.GradientTape() as tape:
        tape.watch(input_image)
        prediction = model(input_image)
        loss = loss_object(input_label, prediction)
    # Get the gradients of the loss w.r.t to the input image.
    gradient = tape.gradient(loss, input_image)
    # Get the sign of the gradients to create the perturbation
    signed_grad = tf.sign(gradient)
    return signed_grad

def fgsm_attack(model, original_image, eps=2.0/255.0):
    """ original_image: preprocessed tensor (batch, h, w, c) BEFORE preprocessing? We used preprocess_input earlier.
        eps: perturbation strength in pixel-value scale [0,1]. For InceptionV3 inputs that are scaled by preprocess_input,
        small eps like 2/255 or 4/255 are common.
    """
    # If original_image is not preprocessed, preprocess it first. Here we assume preprocess_input already applied.
    # Get the model's prediction to construct a one-hot label for the highest-confidence class
    preds = model.predict(original_image)
    class_idx = np.argmax(preds[0])
    one_hot = tf.one_hot([class_idx], preds.shape[-1])

    perturbations = create_adversarial_pattern(model, tf.cast(original_image, tf.float32), tf.cast(one_hot, tf.float32))
    adv_x = original_image + eps * perturbations
    # InceptionV3 preprocess_input expects inputs in range [-1,1] (it scales), so clip appropriately
    adv_x = tf.clip_by_value(adv_x, -1.0, 1.0)
    return adv_x, class_idx

# Craft adversarial example with different epsilons and show predictions
eps_values = [0.0, 2.0/255.0, 4.0/255.0, 8.0/255.0]
results = []
for eps in eps_values:
    adv, orig_class = fgsm_attack(model, x_proc, eps=eps)
    preds_adv = model.predict(adv)
    top = decode_predictions(preds_adv, top=3)[0]
    results.append((eps, orig_class, top, adv.numpy()))

fig, axes = plt.subplots(1, len(results), figsize=(4*len(results),4))
if len(results)==1:
    axes=[axes]
for ax, (eps, orig_class, top, adv_np) in zip(axes, results):
    # Convert adversarial image back to displayable uint8
    # Inception preprocess_input maps pixels to [-1,1]; bring back to [0,255]
    img_disp = ((adv_np[0] + 1.0) * 127.5).astype(np.uint8)
    ax.imshow(img_disp)
    ax.axis('off')
    ax.set_title(f'eps={eps:.3f}\nTop: {top[0][1]} ({top[0][2]*100:.1f}%)')

plt.show()

print('Detailed predictions:')
for eps, orig_class, top, _ in results:
    print(f'epsilon={eps:.4f} -> top1: {top[0][1]} ({top[0][2]*100:.2f}%), top2: {top[1][1]} ({top[1][2]*100:.2f}%)')

"""### Notes & next steps
- Try different images (change `img_url`).
- Use targeted attacks by constructing `one_hot` for a target class instead of the predicted class.
- Try stronger attacks (PGD) or implement defenses like adversarial training.
- If running in Colab, enable GPU runtime (Runtime → Change runtime type → GPU) for faster gradients.
"""